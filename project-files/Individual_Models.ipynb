{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#!pip install streamlit\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import random\r\n",
    "from sklearn.linear_model import LinearRegression\r\n",
    "import xlsxwriter\r\n",
    "\r\n",
    "# Clustering\r\n",
    "from sklearn.cluster import KMeans\r\n",
    "# from sklearn.metrics import silhouette_samples, silhouette_score\r\n",
    "from operator import itemgetter\r\n",
    "# !pip install pyclustering\r\n",
    "from pyclustering.cluster.kmeans import kmeans\r\n",
    "from pyclustering.utils.metric import type_metric, distance_metric\r\n",
    "from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer\r\n",
    "from scipy.spatial import distance_matrix\r\n",
    "from scipy.spatial import distance\r\n",
    "\r\n",
    "# Principal Components Analysis\r\n",
    "from scipy import stats\r\n",
    "# from sklearn.decomposition import PCA\r\n",
    "\r\n",
    "# Classification\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "import itertools"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Import Data\r\n",
    "df = pd.read_csv('..\\Test-Data\\dgn_raw_data.csv')\r\n",
    "\r\n",
    "# Add very small random number to Rating\r\n",
    "df['target']=df['Rating'].apply(lambda x: x+random.random()/1000)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Create a Pandas Excel writer using XlsxWriter as the engine.\r\n",
    "writer = pd.ExcelWriter('pandas_multiple.xlsx', engine='xlsxwriter')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Regressions for Each UID"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Unique IDs\r\n",
    "ids = df.UID.unique()\r\n",
    "\r\n",
    "# Run linear regressions for each UID\r\n",
    "op = pd.DataFrame\r\n",
    "intercept = []\r\n",
    "coefficients=[]\r\n",
    "UID = []\r\n",
    "for p in ids:\r\n",
    "    df_i = df[df.UID == p]              # Create dataframe for current user id\r\n",
    "    X = df_i.filter(regex='^[a-zA-Z][0-9]')  # df input variables only\r\n",
    "    y = df_i['target']                  # Series of target variable\r\n",
    "    reg = LinearRegression().fit(X, y)  # Fit linear regression\r\n",
    "    reg.score(X, y)                     # Score regression model\r\n",
    "    unique_id=df_i['UID'].unique()      # Saves current user id\r\n",
    "    const = reg.intercept_              # Save intercept of the regression model\r\n",
    "    coef = reg.coef_                    # Coefficients of regression model\r\n",
    "    UID.append(unique_id)               # Append current user id\r\n",
    "    intercept.append(const)             # Append current intercept\r\n",
    "    coefficients.append(coef)           # Append current regression coefficients\r\n",
    "\r\n",
    "# Convert newly created lists into dataframes\r\n",
    "intercep_new = pd.DataFrame(intercept)\r\n",
    "coefficients_new = pd.DataFrame(coefficients)\r\n",
    "UID_new = pd.DataFrame(UID)\r\n",
    "\r\n",
    "# Get columns names\r\n",
    "colNames = df.drop(['Rating', 'target',], axis=1).columns\r\n",
    "colNames = colNames.insert(1, 'Const')\r\n",
    "colNames\r\n",
    "\r\n",
    "# Concatenate the new dataframes and add column names\r\n",
    "op = pd.concat([UID_new,intercep_new, coefficients_new], axis=1)\r\n",
    "op.columns = colNames\r\n",
    "\r\n",
    "# Save only regression coefficients for clustering\r\n",
    "scores = op.drop(['UID','Const'], axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def pearson_dist(x, y):\r\n",
    "    r = stats.pearsonr(x, y)[0]\r\n",
    "    return (1 - r) / 2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cluster on Regression Coefficients"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Create dataframe for storing all cluster/variable combo averages and stdevs\r\n",
    "cls_averages_all = pd.DataFrame()\r\n",
    "\r\n",
    "# All maps for all cluster solutions\r\n",
    "all_maps = []\r\n",
    "\r\n",
    "\r\n",
    "####################################################\r\n",
    "\r\n",
    "# Holds only final cluster solutions\r\n",
    "cluster_solutions = {}\r\n",
    "\r\n",
    "max_clusters = 6\r\n",
    "\r\n",
    "for n in range(2, max_clusters+1):\r\n",
    "\r\n",
    "    # change your df to numpy arr\r\n",
    "    sample = scores.to_numpy()\r\n",
    "    \r\n",
    "    # define a custom metric\r\n",
    "    metric = distance_metric(type_metric.USER_DEFINED, func=pearson_dist)\r\n",
    "    \r\n",
    "    # carry out a km++ init\r\n",
    "    initial_centers = kmeans_plusplus_initializer(sample, n, random_state=123).initialize()\r\n",
    "    \r\n",
    "    # execute kmeans\r\n",
    "    kmeans_instance = kmeans(sample, initial_centers, metric=metric)\r\n",
    "    \r\n",
    "    # run cluster analysis\r\n",
    "    kmeans_instance.process()\r\n",
    "    \r\n",
    "    # get clusters\r\n",
    "    clusters = kmeans_instance.get_clusters()\r\n",
    "    \r\n",
    "    # Empty dataframe to take in cluster assignments for each loop iteration\r\n",
    "    df_clusters = pd.DataFrame()\r\n",
    "\r\n",
    "    for i in range(len(clusters)):\r\n",
    "        df_scores = scores.iloc[clusters[i],:]\r\n",
    "        df[f'Optimal {n} cluster solution'] = i+1\r\n",
    "        df_clusters = pd.concat([df_clusters, df_scores])\r\n",
    "        df_clusters.sort_index(inplace=True)\r\n",
    "    \r\n",
    "    cluster_solutions[f'Optimal {n} cluster solution'] = df_clusters.iloc[:, -1]\r\n",
    "\r\n",
    "all_cluster_solutions = pd.DataFrame.from_dict(cluster_solutions)\r\n",
    "\r\n",
    "op = op.merge(all_cluster_solutions, left_index=True, right_index=True)\r\n",
    "\r\n",
    "####################################################"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "#**********************************************************************#\r\n",
    "# This is where the classification stuff begins\r\n",
    "\r\n",
    "# Column of cluster solutions\r\n",
    "last_var = op.shape[1]-max_clusters+1   # 18\r\n",
    "last_cluster = op.shape[1]  # 23"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "#**********************************************************************#\r\n",
    "# Average and Standard Deviations for each cluster/variable combination\r\n",
    "# For cluster 1 of 2, calculate the average and stdev for each variable\r\n",
    "# For cluster 2 of 2, calculate the average and stdev for each variable\r\n",
    "# Etc.\r\n",
    "\r\n",
    "for i in range(last_var, last_cluster):\r\n",
    "    \r\n",
    "    df_cl = op.iloc[:,np.r_[2:last_var,i]]  # i is the current cluster solution\r\n",
    "    df_cl_cons = op.iloc[:,np.r_[1:last_var,i]]  # Same as df_cl but with constant\r\n",
    "\r\n",
    "    if n == max_clusters:\r\n",
    "\r\n",
    "        cls_avg_list = []\r\n",
    "\r\n",
    "        # Take the mean of every variable for each cluster\r\n",
    "        for k in range(1, int(df_cl_cons.iloc[:,-1].max()+1)):\r\n",
    "            cls_mean = df_cl_cons[df_cl_cons.iloc[:,-1] == k].iloc[:,0:-1].mean()\r\n",
    "            cls_mean = cls_mean.append(pd.Series({\"Count\":df_cl[df_cl.iloc[:,-1] == k].iloc[:,0:-1].shape[0]}))\r\n",
    "            cls_avg_list.append(cls_mean)\r\n",
    "            cls_std = df_cl_cons[df_cl_cons.iloc[:,-1] == k].iloc[:,0:-1].std()\r\n",
    "            cls_avg_list.append(cls_std)\r\n",
    "            # NaN means there is either only 1 observation in that cluster or none.\r\n",
    "\r\n",
    "        # Convert to dataframe and transpose\r\n",
    "        cls_averages = pd.DataFrame(cls_avg_list)\r\n",
    "        cls_averages = cls_averages.T\r\n",
    "\r\n",
    "        # Create helpful column names (Cluster # of total_#)\r\n",
    "        col_names = []\r\n",
    "\r\n",
    "        for col in range(1, k+1):\r\n",
    "            new_name1 = f\"Avg cluster {col}/{k}\"\r\n",
    "            col_names.append(new_name1)\r\n",
    "            new_name2 = f\"Std cluster {col}/{k}\"\r\n",
    "            col_names.append(new_name2)            \r\n",
    "\r\n",
    "        # Rename columns\r\n",
    "        cls_averages.columns = col_names\r\n",
    "\r\n",
    "        cls_averages_all = pd.concat([cls_averages_all, cls_averages], axis=1)\r\n",
    "\r\n",
    "op.to_excel(writer, index=False, sheet_name=\"All Regressions, Clusters\")\r\n",
    "\r\n",
    "\r\n",
    "# Add averages for all observations to cls_averages_all before exporting\r\n",
    "all_obs = []\r\n",
    "\r\n",
    "# Variable means for all observations\r\n",
    "all_obs_mean = list(op.filter(regex='^[a-zA-Z][0-9]').mean().values)\r\n",
    "all_obs_mean.insert(0,op['Const'].mean())\r\n",
    "all_obs.append(all_obs_mean)\r\n",
    "\r\n",
    "# cls_averages_all['new_col'] = pd.Series(list(op.filter(regex='^[a-zA-Z][0-9]').mean().values))\r\n",
    "\r\n",
    "# Variable standard deviations for all observations\r\n",
    "all_obs_std = list(op.filter(regex='^[a-zA-Z][0-9]').std().values)\r\n",
    "all_obs_std.insert(0,op['Const'].std())\r\n",
    "all_obs.append(all_obs_std)\r\n",
    "\r\n",
    "\r\n",
    "# Save as dataframe and append to all cls_averages_all dataframe\r\n",
    "all_obs_cols = list(op.filter(regex='^[a-zA-Z][0-9]').columns)\r\n",
    "all_obs_cols.insert(0, \"Const\")\r\n",
    "all_obs_df = pd.DataFrame(all_obs, columns=all_obs_cols)\r\n",
    "all_obs_df = all_obs_df.T\r\n",
    "all_obs_cols = ['All obs avg', 'All obs stdev']\r\n",
    "all_obs_df.columns = all_obs_cols\r\n",
    "cls_averages_all = pd.concat([cls_averages_all, all_obs_df], axis=1)\r\n",
    "\r\n",
    "\r\n",
    "cls_averages_all.to_excel(writer, sheet_name=\"Cluster Avgs and StDevs\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "writer.save()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## End of script."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('market-research': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "interpreter": {
   "hash": "98aa36c4cd3202046900e4a598629031879022659974337312d054f4df87d4d8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}